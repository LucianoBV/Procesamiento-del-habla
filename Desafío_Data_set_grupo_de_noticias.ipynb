{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9Ik2izY17PTHJJ1oWhhnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucianoBV/Procesamiento-del-habla/blob/main/Desaf%C3%ADo_Data_set_grupo_de_noticias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ech9qJaUo9vK"
      },
      "outputs": [],
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzBp-n2Odq2D"
      },
      "outputs": [],
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes parámetros de instanciación en la documentación de sklearn\n",
        "tfidfvect = TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ftPlyanuak8n",
        "outputId": "7d1b2c99-aae1-4b48-c5ff-61060600679c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# en el atributo `data` accedemos al texto\n",
        "newsgroups_train.data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zxcXV6aC_oL"
      },
      "outputs": [],
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador\n",
        "# (obtener el vocabulario y calcular el vector IDF)\n",
        "# y transformar directamente los datos\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-término"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sv7TXbda41-",
        "outputId": "fba2c11a-e5c3-473d-d421-e2115f211a34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape: (11314, 101631)\n",
            "cantidad de documentos: 11314\n",
            "tamaño del vocabulario (dimensionalidad de los vectores): 101631\n"
          ]
        }
      ],
      "source": [
        "# recordar que las vectorizaciones por conteos son esparsas\n",
        "# por ello sklearn convenientemente devuelve los vectores de documentos\n",
        "# como matrices esparsas\n",
        "print(type(X_train))\n",
        "print(f'shape: {X_train.shape}')\n",
        "print(f'cantidad de documentos: {X_train.shape[0]}')\n",
        "print(f'tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# busquemos el conjunto de datos de 20 grupos de noticias y comencemos a analizar los documentos.\n",
        "newsgroups = fetch_20newsgroups(subset='all')\n",
        "newsgroups.target_names[:5], len(newsgroups.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6T5bbbhcTmT",
        "outputId": "998cb0f6-256c-459f-d5ed-7f98e6f99620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['alt.atheism',\n",
              "  'comp.graphics',\n",
              "  'comp.os.ms-windows.misc',\n",
              "  'comp.sys.ibm.pc.hardware',\n",
              "  'comp.sys.mac.hardware'],\n",
              " 18846)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Seleccionamos 5 documentos al azar\n",
        "random.seed(42)  # Fijamos la semilla para obtener resultados reproducibles\n",
        "random_docs_indices = random.sample(range(len(newsgroups.data)), 5)\n",
        "random_docs = [newsgroups.data[i] for i in random_docs_indices]\n"
      ],
      "metadata": {
        "id": "wszwIuYMesXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizamos el texto de todos los documentos\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(newsgroups.data)"
      ],
      "metadata": {
        "id": "0xFb3e9dezTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos la similitud de coseno entre los 5 documentos seleccionados y el resto\n",
        "similarity_matrix = cosine_similarity(X_tfidf[random_docs_indices], X_tfidf)"
      ],
      "metadata": {
        "id": "kSPGENa0e4Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para cada documento, encontramos los 5 documentos más similares (excluyendo el propio documento)\n",
        "most_similar_docs = [(-similarity_matrix[i, :]).argsort()[1:6] for i in range(5)]\n"
      ],
      "metadata": {
        "id": "IAJ5vZcFe-Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un resumen con los documentos seleccionados y sus 5 más similares\n",
        "similarity_summary = []\n",
        "for i, doc_idx in enumerate(random_docs_indices):\n",
        "    doc_info = {\n",
        "        'Selected Document Index': doc_idx,\n",
        "        'Selected Document Category': newsgroups.target_names[newsgroups.target[doc_idx]],\n",
        "        'Most Similar Documents Indices': most_similar_docs[i],\n",
        "        'Most Similar Documents Categories': [newsgroups.target_names[newsgroups.target[idx]] for idx in most_similar_docs[i]],\n",
        "    }\n",
        "    similarity_summary.append(doc_info)"
      ],
      "metadata": {
        "id": "VY0EhFdEe4lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pbsjm2Civjh",
        "outputId": "0f7ee177-1a68-454c-e98a-2b650c90cc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Selected Document Index': 7314, 'Selected Document Category': 'rec.motorcycles', 'Most Similar Documents Indices': array([ 1142, 13291, 13151,   785, 11568]), 'Most Similar Documents Categories': ['rec.motorcycles', 'misc.forsale', 'rec.motorcycles', 'rec.autos', 'talk.politics.guns']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a entrenar los modelos Naïve Bayes (MultinomialNB y ComplementNB) y optimizar su rendimiento\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Separar en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, newsgroups.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entrenamos el modelo Multinomial Naive Bayes\n",
        "multinomial_nb = MultinomialNB()\n",
        "multinomial_nb.fit(X_train, y_train)\n",
        "y_pred_mnb = multinomial_nb.predict(X_test)\n",
        "f1_mnb = f1_score(y_test, y_pred_mnb, average='macro')\n",
        "\n",
        "# Entrenamos el modelo Complement Naive Bayes\n",
        "complement_nb = ComplementNB()\n",
        "complement_nb.fit(X_train, y_train)\n",
        "y_pred_cnb = complement_nb.predict(X_test)\n",
        "f1_cnb = f1_score(y_test, y_pred_cnb, average='macro')\n",
        "print(f1_mnb, f1_cnb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa4xljGuk5u-",
        "outputId": "30dea046-9a71-4384-c63e-ab8e5e003f6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8663399374130707 0.8964060472437889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estudiemos la similaridad entre palabras tomando manualmente 5 palabras del vocabulario del dataset\n",
        "# Primero obtendremos las palabras más frecuentes del vocabulario para seleccionar palabras relevantes\n",
        "\n",
        "# Obtener el vocabulario y la frecuencia de las palabras\n",
        "word_counts = np.asarray(X_tfidf.sum(axis=0)).flatten()\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Crear un dataframe con las palabras y sus frecuencias\n",
        "word_freq_df = pd.DataFrame({'Word': vocab, 'Frequency': word_counts}).sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "# Seleccionaremos palabras comunes y relevantes manualmente para evitar términos poco interpretables\n",
        "selected_words = ['computer', 'windows', 'game', 'science', 'car']\n",
        "\n",
        "# Encontramos los índices de estas palabras en el vocabulario\n",
        "selected_word_indices = [np.where(vocab == word)[0][0] for word in selected_words]\n",
        "\n",
        "# Calculamos la similitud de coseno entre las palabras seleccionadas y el resto del vocabulario\n",
        "word_similarity_matrix = cosine_similarity(X_tfidf.T[selected_word_indices], X_tfidf.T)\n",
        "\n",
        "# Para cada palabra, encontramos las 5 más similares\n",
        "most_similar_words = [(-word_similarity_matrix[i, :]).argsort()[1:6] for i in range(5)]\n",
        "\n",
        "# Creamos un resumen con las palabras seleccionadas y sus 5 más similares\n",
        "word_similarity_summary = []\n",
        "for i, word in enumerate(selected_words):\n",
        "    word_info = {\n",
        "        'Selected Word': word,\n",
        "        'Most Similar Words': [vocab[idx] for idx in most_similar_words[i]]\n",
        "    }\n",
        "    word_similarity_summary.append(word_info)\n"
      ],
      "metadata": {
        "id": "36_egI-ooK0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el resumen de similaridad entre palabras\n",
        "word_similarity_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuLvYqZFqTnw",
        "outputId": "ab66bf93-ee3f-481a-960b-f4a1535d8fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Selected Word': 'computer',\n",
              "  'Most Similar Words': ['science',\n",
              "   'lines',\n",
              "   'subject',\n",
              "   'organization',\n",
              "   'edu']},\n",
              " {'Selected Word': 'windows',\n",
              "  'Most Similar Words': ['dos', 'ms', 'os', 'file', 'run']},\n",
              " {'Selected Word': 'game',\n",
              "  'Most Similar Words': ['games', 'espn', 'baseball', 'hockey', 'score']},\n",
              " {'Selected Word': 'science',\n",
              "  'Most Similar Words': ['computer', 'uhunix', 'scientific', 'uhcc', 'cs']},\n",
              " {'Selected Word': 'car',\n",
              "  'Most Similar Words': ['cars', 'mileage', 'dealer', 'saftey', 'engine']}]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se seleccionaron 5 documentos al azar y se midió su similitud con el resto de los documentos.\n",
        "Se entrenaron dos modelos de clasificación Naïve Bayes: MultinomialNB y ComplementNB. El modelo ComplementNB mostro un mejor resultado con un F1-score de 0.896, superando al MultinomialNB que obtuvo un F1-score de 0.866.\n",
        "Al analizar la similitud entre 5 palabras seleccionadas computer, windows, game, science, y car, se encontró que las palabras más similares tienen una relación sclara. Por ejemplo, game está relacionado con términos como games, baseball, y hockey."
      ],
      "metadata": {
        "id": "WgNZexPXrPdu"
      }
    }
  ]
}